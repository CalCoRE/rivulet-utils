{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Access Notebook for OpenAQ\n",
    "_by Michelle H Wilkerson, Adelmo Eloy_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Purpose of this Notebook\n",
    "\n",
    "This notebook was developed as part of NSF Grant 2445609 to support accessing and processing OpenAQ data for middle and high school classroom activities. It's written to be relatively accessible to beginners, but if you have not interacted with computational notebooks or python before you may find navigating this tool difficult. (Check out the Show Your Work project for a gentle introduction to computational notebooks for educators!)\n",
    "\n",
    "Our project is focused on supporting data analysis and mechanistic reasoning in science education. In other words, we want students to learn how data provides information about _how scientific mechanisms work_, and how understanding scientific mechanisms can help them to _explain and interpret patterns in data_. This builds on a long history of research on complex systems and agent-based modeling, and more closely connects that work to current expansions of data analysis across subjects.\n",
    "\n",
    "Here, we are focused on Air Quality as a phenomenon. While most students understand that poor Air Quality can impact health, they may not know that there are many different kinds of air pollution, each caused by different processes and chemicals. These are reflected by different patterns over the course of a day or year\n",
    "\n",
    "This data tool allows users to connect to OpenAQ, search for air quality data streams in an area of interest, and then identifies the data streams that record observations for _both_ PM2.5 and O3, as two key pollutants impacting air quality and that tend to behave very differently over time. These kinds of datasets can serve as a launch to examining what AQ is and what are its underlying mechanistic and compositional complexities.\n",
    "\n",
    "You are welcome to modify and adapt this script. You may find the OpenAQ documentation [here](https://docs.openaq.org/) and [here](https://python.openaq.org/) helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Part I: Connecting with OpenAQ\n",
    "\n",
    "Before you get started, you will need an OpenAQ API Key. To get one, register for a free account here. Once you're logged in, go to your account settings. At the bottom of the page you will find an API Key. Copy it and set API_KEY in the cell block below to your key. Then, run the cell below to install the openaq API and set your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openaq\n",
    "\n",
    "API_KEY = \"paste your key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Now we'll open your openAQ client. This is what talks to openAQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the client to get data from openAQ\n",
    "\n",
    "from openaq import OpenAQ\n",
    "\n",
    "client = OpenAQ(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "When we make a request through the client, it will pass data and information back to us in http and JSON formats. These are standard text, but they are not formatted in order to save space. This makes it hard to read and understand the structure of the info we're getting from the API. So the cell below loads some helper tools to process and \"pretty print\" the information we get from OpenAQ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some helpful tools for looking at data\n",
    "\n",
    "import json # to handle JSON\n",
    "from pprint import pprint # pretty print JSON and other structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We are going to focus on datasets that report levels of PM2.5 and O3 for the same location. But, you might be interested in accessing data about other pollutants. The cell below lists all the site-specific parameters that are available through the API. This doesn't mean that each site has each parameter, it just reviews everything that can be requested (whether or not it exists for a given site or date range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see all the parameters we can access from openaq\n",
    "pprint(client.parameters.list().results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The list above shows that there are a few parameters that report PM2.5 and O3, some measured in different ways. The parameters with ids 2, 97, 130, and 19860 all report PM2.5 in different ways. The parameters with ids 3 and 10 report O3 using µg/m³ and ppm, respectively. \n",
    "\n",
    "After some tinkering, it seems that a lot of stations use sensor id 10 (reporting O3 in ppm). This is not shown on the web explorer, but is still data we probably want to consider using in our work. Nearly all the sensors include id 2 (PM2.3 in µg/m³). So for now, I'm just going to go with parameter 2 and then _either_ 3 or 10, to catch as many instances of PM2.5 and O3 as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Part 2: Finding a location that meets our sensor needs\n",
    "\n",
    "Ok so now let's narrow in on a location. Eventually, if we want, we can use geopy to look up the lat and long coordinates for whatever location you prefer. But that requires some signup and so for now I'm gonna hard code and we decide this is useful we can go with geopy or whatever. Then we'll do a search for the nearest OpenAQ data stream to the specified lat and long that has recent readings for _both_ PM2.5 and O3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF WE WANT TO USE GEOPY WE WOULD USE THE CODE BELOW\n",
    "\n",
    "# pip install geopy\n",
    "# from geopy import geocoders\n",
    "# gn = geocoders.GeoNames()\n",
    "\n",
    "# gn.geocode(\"Cleveland, OH 44106\") \n",
    "# then we'd take the returned object and feed lat and long to next step\n",
    "\n",
    "# fetch the locations near the specified lat and long\n",
    "fetch_locations = json.loads(client.locations.list(coordinates=[37.85, -122.25], radius=12000, limit=1000).json())\n",
    "locations = fetch_locations[\"results\"]\n",
    "\n",
    "# report how many locations we found\n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We likely found more locations than we expected! But, these are not all likely to have the specific information we need. Let's filter our results so we're only looking at locations that PM2.5 and O3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_locations = []\n",
    "\n",
    "for location in locations:\n",
    "    #reset flags for each location\n",
    "    pm = False\n",
    "    o3 = False \n",
    "\n",
    "    #see if the pollutants are recorded\n",
    "    for sensor in location['sensors']:\n",
    "        if sensor['parameter']['id'] == 2:\n",
    "            pm = True\n",
    "        if sensor['parameter']['id'] == 3 or sensor['parameter']['id'] == 10:\n",
    "            o3 = True\n",
    "\n",
    "    #if both pollutants are recorded, add to the list\n",
    "    if pm & o3:\n",
    "        filtered_locations.append(location)\n",
    "\n",
    "# kind of a hack, but let's go ahead and load pandas so we can display this nicely\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(filtered_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Good! Now you can clearly see the id of stations that have the data you need. You can also see the interval of times for which these records are available by scrolling all the way to the right in the table above. If you're using the originally hard-coded bay area, these look like they both started in 2016 and are up-to-date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Part 3: Loading your data\n",
    "\n",
    "Okay, now you've found a place you can load some data that might exhibit patterns that will be intriguing to students and that will highlight some of our learning goals around the complex nature of AQ. Let's look at recent data for the location with id 2135, identified above. The API has us pull measurements per sensor, so let's look for the sensor ids we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(client.locations.sensors(locations_id=2135).results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Ok so our O3 sensor id is 3833, and our PM2.5 sensor is 3835. Let's fetch the latest data for each of these sensors. We clean it up and merge it into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3 = client.measurements.list(sensors_id=\"3833\")\n",
    "pm = client.measurements.list(sensors_id=\"3835\")\n",
    "\n",
    "o3_df = pd.DataFrame(o3.results)\n",
    "pm_df = pd.DataFrame(pm.results)\n",
    "\n",
    "def cleanup(messy_df, pollutant):\n",
    "    df = pd.DataFrame(messy_df)\n",
    "\n",
    "    # Extract 'local' datetime strings into new columns\n",
    "    df['datetime_from'] = df['period'].str.get('datetime_from').str.get('local')\n",
    "    df['datetime_to']   = df['period'].str.get('datetime_to').str.get('local')\n",
    "\n",
    "    # Convert to pandas datetimes\n",
    "    df['datetime_from'] = pd.to_datetime(df['datetime_from'])\n",
    "    df['datetime_to']   = pd.to_datetime(df['datetime_to'])\n",
    "\n",
    "    # Tag with pollutant label\n",
    "    df['pollutant'] = pollutant\n",
    "\n",
    "    # Drop unused columns if present\n",
    "    for col in ['period', 'parameter', 'coordinates', 'summary', 'coverage']:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "    return df\n",
    "\n",
    "def fetch_1000_since(client, sensor_id, start=None):\n",
    "    \"\"\"\n",
    "    Return up to 1000 measurements for a given sensor, starting at `start` (inclusive).\n",
    "    Strategy:\n",
    "      1) Try common server-side date params (APIs differ in naming).\n",
    "      2) If none works, paginate client-side and filter by datetime_from.local.\n",
    "    \"\"\"\n",
    "    base = {\"sensors_id\": str(sensor_id), \"limit\": 1000}\n",
    "\n",
    "    # Normalize `start` to ISO string and timestamp\n",
    "    start_ts = pd.to_datetime(start) if start else None\n",
    "    start_iso = start_ts.strftime(\"%Y-%m-%dT%H:%M:%SZ\") if start_ts is not None else None\n",
    "\n",
    "    # --- (1) Try server-side date filtering with common param names ---\n",
    "    if start_iso is not None:\n",
    "        for key in (\"datetime_from\", \"start_at\", \"start\", \"from_datetime\", \"from_date\", \"date_from\"):\n",
    "            try:\n",
    "                resp = client.measurements.list(**{**base, key: start_iso})\n",
    "                return pd.DataFrame(resp.results[:1000])\n",
    "            except TypeError:\n",
    "                # This API doesn't recognize that param name; try the next one.\n",
    "                continue\n",
    "\n",
    "    # If we reach here, either no start provided or server-side filtering not available.\n",
    "    # We'll paginate and filter on the client side.\n",
    "\n",
    "    collected = []\n",
    "    page = 1\n",
    "    while len(collected) < 1000:\n",
    "        # --- (2) Try page-based pagination; fall back to offset if needed ---\n",
    "        page_resp = None\n",
    "        try:\n",
    "            page_resp = client.measurements.list(**{**base, \"page\": page})\n",
    "        except TypeError:\n",
    "            try:\n",
    "                page_resp = client.measurements.list(**{**base, \"offset\": (page - 1) * base[\"limit\"]})\n",
    "            except TypeError:\n",
    "                # No pagination supported by params; use a single page once.\n",
    "                if page == 1:\n",
    "                    page_resp = client.measurements.list(**base)\n",
    "                else:\n",
    "                    page_resp = None\n",
    "\n",
    "        if page_resp is None:\n",
    "            break\n",
    "\n",
    "        items = getattr(page_resp, \"results\", []) or []\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        # Filter items by start_ts using the 'period.datetime_from.local' field when available\n",
    "        for r in items:\n",
    "            dt_local = None\n",
    "            if isinstance(r, dict):\n",
    "                p = r.get(\"period\")\n",
    "                if isinstance(p, dict):\n",
    "                    df = p.get(\"datetime_from\")\n",
    "                    if isinstance(df, dict):\n",
    "                        dt_local = df.get(\"local\")\n",
    "                # Fallback for APIs that use 'date.local'\n",
    "                if dt_local is None:\n",
    "                    d = r.get(\"date\")\n",
    "                    if isinstance(d, dict):\n",
    "                        dt_local = d.get(\"local\")\n",
    "\n",
    "            # Keep if no start filter OR datetime exists and is >= start\n",
    "            if (start_ts is None) or (dt_local and pd.to_datetime(dt_local) >= start_ts):\n",
    "                collected.append(r)\n",
    "                if len(collected) >= 1000:\n",
    "                    break\n",
    "\n",
    "        # Stop if this looks like the last page\n",
    "        if len(items) < base[\"limit\"]:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    return pd.DataFrame(collected[:1000])\n",
    "\n",
    "# ===== Usage: set your desired start date (YYYY-MM-DD or ISO8601) =====\n",
    "start = \"2025-01-01\"  # <-- change this to control the start of the collection\n",
    "\n",
    "o3_raw = fetch_1000_since(client, 3833, start=start)\n",
    "pm_raw = fetch_1000_since(client, 3835, start=start)\n",
    "\n",
    "o3_df = cleanup(o3_raw, \"O3\")\n",
    "pm_df = cleanup(pm_raw, \"PM\")\n",
    "\n",
    "merged = pd.concat([o3_df, pm_df], ignore_index=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Now we're ready to plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "line_plot = sns.lineplot(\n",
    "    data=o3_df,\n",
    "    x='datetime_from',\n",
    "    y='value',\n",
    "    hue='pollutant',\n",
    "    palette='viridis' # Use a nice color palette\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_plot = sns.lineplot(\n",
    "    data=pm_df,\n",
    "    x='datetime_from',\n",
    "    y='value',\n",
    "    hue='pollutant',\n",
    "    palette='viridis' # Use a nice color palette\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Here's some code to expore a csv of the merged dataset to use in other tools. (Should be very easy to work with in CODAP from here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(\"filename.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
